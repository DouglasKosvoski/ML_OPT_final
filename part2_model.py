# -*- coding: utf-8 -*-
"""Part2_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lh9XCswYq1jC1C5ykyKOAp5ByIw2b4xi

Douglas Kosvoski
    1911100022

# Part 1

## Loading Libraries
"""

!pip install tomotopy spacy > /dev/null

# Commented out IPython magic to ensure Python compatibility.
import sys
import spacy
import gensim

import pandas as pd
import tomotopy as tp
import numpy as np
import pandas as pd
import re

import matplotlib.pyplot as plt
# %matplotlib inline

from gensim.models.phrases import Phrases, Phraser
from bs4 import BeautifulSoup

spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

import warnings
warnings.filterwarnings('ignore')

"""## Loading dataset"""

""" The dataset is avaiable through google sheets api """
# https://docs.google.com/spreadsheets/d/1xdU55aEjDM-gdVWOm1UcSPmE0KcMFG5XHzs-9nyBTEA

sheet_id = "1xdU55aEjDM-gdVWOm1UcSPmE0KcMFG5XHzs-9nyBTEA"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet="

df = pd.concat([
  pd.read_csv(url + 'train').drop(['Id', 'CreationDate', 'Y'], axis=1),
  pd.read_csv(url + 'valid').drop(['Id', 'CreationDate', 'Y'], axis=1)
])

df.info()

def get_average_feature_length(docs, feature):
  total_size = 0
  max_size = 0
  min_size = 999

  for i in docs:
    length = len(i)
    total_size += length
  
    if length > max_size:
      max_size = length

    if length < min_size:
      min_size = length
    
  print(f"\n{'-'*10} {feature} {'-'*10}")
  print(f"Average # of {feature}: {int(total_size / len(docs))}")
  print(f"Max # of {feature}: {max_size}")
  print(f"Min # of {feature}: {min_size}")
  return int(total_size / len(docs))

avg_title_size = get_average_feature_length(df['Title'], 'Title')
avg_body_size  = get_average_feature_length(df['Body'], 'Body')
avg_tag_size   = get_average_feature_length(df['Tags'], 'Tags')

number_of_docs = 100
feature = df.Body

docs = list(feature)[:number_of_docs]
print(len(docs))

"""## Remove HTML tags"""

def remove_special_characters(character):
  if character.isalnum() or character in [' ', '.', '\n']:
    return True
  return False

def remove_tags(docs):
  cleaned_docs = []
  tags = r"<pre>|</pre>|<a|</a>|<img>|</img>|href|http"

  count = 0
  for doc in docs:
    print(f"{count} out of {len(docs)}", end="\r")
    split_string = re.split(tags, doc.strip().replace('\n', ''))
    clean_doc = ''

    for i in split_string:
      i = i.strip()
      if not (i.startswith('<code>') or i.startswith('=')):
        text = BeautifulSoup(i.strip(), "lxml").text
        asd = "".join(filter(remove_special_characters, text))
        clean_doc += asd

    cleaned_docs.append(clean_doc.strip())
    count += 1
  return cleaned_docs

docs_without_tags = remove_tags(docs)

for i, t in enumerate(zip(docs_without_tags)):
  if 'character' in t:
    print(t)
    break
  print(i, t)



"""## Lematization"""

def lematize(docs):
  dlemma, total_size = [], len(docs)

  for i, d in enumerate(docs):
    print(f"{i} out of {total_size}", end='')
    lm = " ".join([token.lemma_ for token in nlp(d) if not (token.is_stop == True or token.is_digit == True or token.is_punct == True)])
    dlemma.append(lm.lower())
    print('\r\r\r\r\r\r\r\r', end='')
  return dlemma

dlemma = lematize(docs)

for i in dlemma[:10]:
  print(i)

"""## Tokenization"""

min_length = 4
dtoken = [gensim.utils.simple_preprocess(d, deacc=True, min_len=min_length) for d in dlemma]

for i in dtoken:
  if 'character' in i and 'offset' in i:
    print(i)
    
  # print(i)

"""## N-grams"""

bigram  = Phraser(Phrases(dtoken, min_count=2, threshold=10))
bdocs   = [bigram[d] for d in dtoken]

def get_bigrams(docs):
  bigrams = []
  for doc in bdocs:
    for j in doc:
      if '_' in j and j not in bigrams:
        bigrams.append(j)

  return bigrams

print(get_bigrams(bdocs)[:10])

"""## Metrics"""

def stats_about_the_docs(docs, feature):
  total_size = 0
  max_size = 0
  min_size = 999

  for i in docs:
    length = len(i)
    total_size += length
  
    if length > max_size:
      max_size = length

    if length < min_size and length != 0:
      min_size = length
    
  print(f"{'-'*10} {feature} {'-'*10}")
  print(f"Average # of {feature}: {int(total_size / len(docs))}")
  print(f"Max # of {feature}: {max_size}")
  print(f"Min # of {feature}: {min_size}")

stats_about_the_docs(bdocs, 'Words')

"""## Wordcloud"""

def create_single_string(tokens):
  output = ''
  for i in tokens:
    for j in i:
      output += j + " "

  return output

output = create_single_string(bdocs)
output

import wordcloud as wc
import matplotlib.pyplot as plt

mycloud = wc.WordCloud(prefer_horizontal=1, width=1920, height=1080, min_font_size=6, background_color='white').generate(output)
plt.figure(figsize=(16,9))
plt.imshow(mycloud)

"""# Part 2

## Feature Extraction
"""

def join_docs(docs):
  docs = []

  for i in bdocs:
    for j in i:
      docs.append(j)

  return docs

joined_docs = join_docs(bdocs)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(joined_docs)

features  = vectorizer.get_feature_names_out()
dense     = X.todense()
denseList = dense.tolist()

df = pd.DataFrame(denseList, columns=set(features))
for feature in df.columns:
  print(feature, df[feature].unique())

"""## Model"""

def get_coherence(mdl):
	average_coherence = 0
	for preset in ['c_v']:
		coh = tp.coherence.Coherence(mdl, coherence=preset)
		average_coherence = coh.get_score()
	return average_coherence

def runModel(mdl, docs):
	for i, d in enumerate(docs):
		mdl.add_doc(d)
	
	mdl.burn_in = 100
	mdl.train(0)
 
	for i in range(0, 100, 2):
		mdl.train(10)
	# mdl.save('test.lda.bin', True)

def printTopics(mdl, p=False, top_n=10):
	for k in range(mdl.k):
		print(f" -> Topic #{k}")

		for word, prob in mdl.get_topic_words(topic_id=k, top_n=top_n):
			if p:
				print(f"{'%20s' % word} ({'%.2f' % prob})", end=" ")
			else:
	 			print(f"{word.strip()}", end=" ")
		print()

# tp.LDAModel docs
# bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel

best_alpha, best_beta = None, None
# alpha, beta = [0.1, 0.5, 1, 1.5, 2], alpha.copy()
alpha, beta = 2, 2

mdl, best_model = None, None
cv_scores = []
step = 1

for a in [alpha]:
  for b in [beta]:
    for i in range(1, len(bdocs)+1, step):
      print(f"{i} out of {len(bdocs)}")
      mdl = tp.LDAModel(
        tw      = tp.TermWeight.IDF,
        min_cf  = 3,
        rm_top  = 5,
        k       = i,
        seed    = 777,
        alpha   = a,
        eta     = b
      )

      runModel(mdl, bdocs)
      current_coherence = get_coherence(mdl)

      cv_scores.append(current_coherence)

      if current_coherence >= max(cv_scores):
        print(f"\n{'-'*10} Better params found!!! {'-'*10}")
        print(f"Alpha: {a} \t Beta: {b} \t Score: {'%.4f' % current_coherence}\n")
        best_model = mdl
        best_alpha = a
        best_beta  = b

print(cv_scores)

plt.grid()
x = range(1, len(bdocs)+1, step)
plt.plot(x, cv_scores, marker='o')
plt.figure(figsize=(15, 10))
plt.show()

print(f"\nHighest coherence is doc #{max(zip(cv_scores, x))[1]-1} with {'%.4f' % max(zip(cv_scores, x))[0]}")

"""# Part 3"""

try:
  best_model = tp.LDAModel.load('asd.model')
except Exception as e:
  print("Couldn't load the model")

printTopics(best_model, p=True, top_n=10)

def topics_from_model(mdl, top_n=10):
  list_string = []
  for k in range(10):
    string_doc = '%2d ->' % k

    for word, prob in mdl.get_topic_words(topic_id=k, top_n=top_n):
      string_doc += " " + word
    list_string.append(string_doc.title())
  return list_string

list_string = topics_from_model(best_model, top_n=15)
a = [print(i+'\n') for i in list_string]

def find_topics_association_to_docs(model, threshold=0.5):
  """
    Number of times a topic appears
      e.g. topic # X appears Y times in the docs
  """
  appearances = [0 for i in range(model.k)]

  for index, doc in enumerate(model.docs):
    tokens = doc.get_topics()

    for token in tokens:
      token_number, token_score = token[0], token[1]

      if token_score > threshold:
        appearances[token_number] += 1

  return appearances

association = find_topics_association_to_docs(best_model)
association

def sort_association(assoc):
  import operator

  dic = {}
  for i, j in enumerate(assoc):
    dic[str(i)] = j

  return [int(i[0]) for i in sorted(dic.items(), key=operator.itemgetter(1), reverse=True)]

docs_sorted = sort_association(association)[:10]
docs_sorted

def print_top_topics(model, docs):
  print(f" {'-'*75} Top 10 topic most associated with docs {'-'*75} \n")
  for i in docs[:10]:
    topic = " ".join([i[0] for i in model.get_topic_words(i)])
    print('%2d ' % i, topic.title())

print_top_topics(best_model, docs_sorted)

# def print_top_topics(model, top_n=10):
#   print(f" {'-'*75} Top 10 topic most associated with docs {'-'*75} \n")
#   for i in range(0, top_n, 1):
#     topic = " ".join(['%18s' % i[0] for i in model.get_topic_words(i)])
#     print(i, topic.title())

# print_top_topics(best_model)

labels = [
  "Mobile", #11
  "Image Sprite", #9
  "Linux", #12
  "Windows Server", #16
  "Web Dev", #2
  "HTML", #17
  "Game", #19
  "Vector", #6
  "SGBD Driver", #15
  "Database", #20
]

association = association[:10]
plt.figure(figsize=(15, 8))
plt.grid()
plt.bar(range(len(association)), association, color='teal', edgecolor='black')
plt.ylabel('# of Occurances', fontdict=dict(fontsize=15))
plt.rcParams.update({'font.family':'sans-serif'})

for index, data in enumerate(association):
  plt.text(x=index-0.05, y=data+0.15, s=str(data), fontdict=dict(fontsize=15))
  plt.text(
    x        = index,
    y        = -1,
    s        = labels[index],
    rotation = 45,
    fontdict = dict(fontsize=15),
    horizontalalignment = 'center',
    verticalalignment   = 'center'
  )

plt.tight_layout()
plt.show()

