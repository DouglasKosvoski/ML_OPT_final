# -*- coding: utf-8 -*-
"""NPL_stackoverflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15YS-Ik-lpEbRc_ParBfa4pNSZhhGWTul

# Machine Learning final project
---
### Stackoverflow 60k questions
    Student: Douglas Kosvoski - 1911100022

Obs: **It takes a "*few*" minutes to run**

## Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt

# %matplotlib inline

""" The dataset is avaiable through google sheets api """

# https://docs.google.com/spreadsheets/d/1xdU55aEjDM-gdVWOm1UcSPmE0KcMFG5XHzs-9nyBTEA
sheet_id = "1xdU55aEjDM-gdVWOm1UcSPmE0KcMFG5XHzs-9nyBTEA"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet="

df_data = pd.read_csv(url + 'train').drop(['Id', 'CreationDate'], axis=1)
df_test = pd.read_csv(url + 'valid').drop(['Id', 'CreationDate'], axis=1)

# join train and test into a single dataframe
df = pd.concat([df_test, df_data])

"""### Analyzing"""

df.info()

"""### Dealing with NaN/Null values"""

df.isna().sum()

"""## Charts"""

df.head()

output = []
for i in list(df['Y'].unique()):
  count = (df['Y'] == i).sum()
  output.append(count)

plt.pie(
  output,
  labels  =df['Y'].unique(),
  autopct ='%.0f%%',
  colors  =['cyan', 'orange', 'pink'],
  radius  = 1,
  shadow  = True
)

plt.legend(); plt.show()
print(output)

"""## NLP"""

def get_average_feature_length(docs, feature):
  total_size = 0
  max_size = 0
  min_size = 999

  for i in docs:
    length = len(i)
    total_size += length
  
    if length > max_size:
      max_size = length

    if length < min_size:
      min_size = length
    
  print(f"\n{'-'*15} {feature} {'-'*15}")
  print(f"Average length {int(total_size / len(docs))}")
  print(f"Max length {max_size}")
  print(f"Min length {min_size}")
  return int(total_size / len(docs))

avg_title_size = get_average_feature_length(df['Title'], 'Title')
avg_body_size = get_average_feature_length(df['Body'], 'Body')
avg_tag_size = get_average_feature_length(df['Tags'], 'Tags')

# Convert from html format to plain text

from bs4 import BeautifulSoup as soup

clean_titles, clean_bodies, clean_tags = [], [], []
r = len(df)

for f in df.columns:
  if f == 'Title':
    for i in df[f][:r]:
      clean_titles.append((soup(i, "lxml").text, len(i)))

  elif f == 'Body':
    for i in df[f][:r]:
      clean_bodies.append((soup(i, "lxml").text, len(i)))

  elif f == 'Tags':
    for i in df[f][:r]:
      clean_tags.append((" ".join(i.replace('>', '').split('<')).strip(), len(i)))

# remove objects where body length is greater than the avg body length

new_titles, new_bodies, new_tags = [], [], []

for i in range(r):
  if not clean_bodies[i][1] > avg_body_size:
    new_titles.append(clean_titles[i][0])
    new_bodies.append(clean_bodies[i][0])
    new_tags.append(clean_tags[i][0])

# before converting to plain text and the removal of rows which body length exceed the avg length
df.head(5)

# Rejoin all remaining rows to the DataFrame
clean_df = pd.DataFrame()
clean_df['Title'] = new_titles
clean_df['Body'] = new_bodies
clean_df['Tags'] = new_tags
df = clean_df

# After modifications
df.head(5)

"""## Remove Stopwords and punctuation"""

import spacy

spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

def convert_to_string(df, feature) -> str:
  output = []
  for i in range(len(df[feature])):
    output.append(df[feature][i].strip())
  return output

def remove_stop_words_and_punctuation(text: str) -> list:
  dlemma = []
  for i in text:
    tdoc = nlp(i)
    lm = " ".join([token.lemma_ for token in tdoc  if not(token.is_stop or token.is_digit or token.is_punct)])
    dlemma.append(lm)
  return dlemma


titles = convert_to_string(df, 'Title')
bodies = convert_to_string(df, 'Body')
tags   = convert_to_string(df, 'Tags')

"""## bigram"""

from gensim.models.phrases import Phrases, Phraser

import gensim.utils

dtoken  = [gensim.utils.simple_preprocess(d, deacc= True, min_len=3) for d in titles]
phrases = Phrases(dtoken, min_count=2, threshold=9)
bigram  = Phraser(phrases)
bdocs   = [bigram[d] for d in dtoken]

asd = []
for i in bdocs:
  for j in i:
    asd.append(j)
bigranized = asd

bigrams_list = [i for i in bigranized if '_' in i]
print(len(bigrams_list), bigrams_list[:20])

"""## Remove dups"""

uniques = set(bigranized)
uniques = dict(zip(range(len(uniques)), uniques))

print(len(bigranized), len(uniques))

uniques.values()

"""## Tokenize"""

def tokenize(dlemma):
  max_size = len(dlemma); count = 0
  output = []
  
  for i in dlemma:
    if count % 200 == 0:
      print(f"{count}/{max_size}")
    
    aux = []; doc = nlp(i)

    for token in doc:
      if token.pos_ in ['NOUN', 'PROPN']:
        aux.append(token.lemma_)
    output.append(aux)
    count += 1
  return output
        
tokens = tokenize(uniques.values())
tokens[:10]

# remove empty objects from the tokens

print("Before: ", len(tokens))
clean_tokens = []
for t in tokens:
  if t != []:
    clean_tokens.append(t)
tokens = clean_tokens
print("After: ", len(tokens))

tokens

"""## Metrics"""

number_of_docs = len(tokens)
number_of_docs

def stats_about_the_docs(docs):
  shortest_doc, largest_doc = "_"*100, ""
  max_size, min_size = 0, 999
  total_size = 0

  for i in docs:
      length = len(i[0])
      total_size += length

      if length > max_size:
        max_size = length
        largest_doc = i[0]

      if length < min_size:
        min_size = length
        shortest_doc = i[0]

  print(f"Average word length: {int(total_size / len(docs))}")
  print(f"\nMax length {max_size} : {largest_doc}")
  print(f"Min length  {min_size} : {shortest_doc}")

stats_about_the_docs(tokens)

def create_single_string(tokens):
  output = ''
  for i in tokens:
    for j in i:
      output += j + " "

  return output

output = create_single_string(tokens)
output

"""## Wordcloud"""

import wordcloud as wc
import matplotlib.pyplot as plt

mycloud = wc.WordCloud().generate(output)
plt.figure(figsize=(20,10))
plt.imshow(mycloud)